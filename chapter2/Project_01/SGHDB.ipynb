{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection. \n",
    "The dataset includes detailed information on resale transactions of HDB flats between January 1, 2017, and March 30, 2024. The dataset contains 180,154 rows and 11 columns and was downloaded on May 23, 2024. \n",
    "\n",
    "**Dataset URL**: [HDB Resale Prices](https://beta.data.gov.sg/datasets/d_8b84c4ee58e3cfc0ece0d773c8ca6abc/view) \n",
    "**HDB RPI URL**:[HDB Resale Prices Index](https://www.hdb.gov.sg/residential/selling-a-flat/overview/resale-statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_config as cfg\n",
    "\n",
    "df_raw = pd.read_csv('./data/SGHDB2017-2024.csv')\n",
    "cfg.save_dataset_info(df_raw, fname='dataset_info.csv')\n",
    "\n",
    "df = cfg.adjust_resale_price(df_raw, cut_off_date='2024-04-01')\n",
    "cfg.visualize_adjusted_price(df[['month', 'resale_price', 'adjusted_price']].copy(), fname='adjusted_price.png')\n",
    "\n",
    "df_clean = cfg.preprocess_data(df)\n",
    "print(df_raw.shape, df.shape, df_clean.shape)\n",
    "df_clean.to_csv('./data/SGHDB2017-2024_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Required Libraries: \n",
    "Ensure you have the necessary libraries installed. You can install them using pip if they are not already installed.\n",
    "```sgh \n",
    "pip install tensorflow pandas scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: \n",
    "The project starts by importing the pandas library and loading a cleaned CSV file named {SGHDB2017-2024_clean.csv} into a Pandas DataFrame $df$. Then, the {adjusted_price} column, which represents the housing prices to be predicted, is extracted and stored in the variable $y$ and the remaining columns, which serve as features for the model, are stored in the variable $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/SGHDB2017-2024_clean.csv')\n",
    "y = df['adjusted_price'].values   # Target\n",
    "X = df.drop(columns = 'adjusted_price')  # Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the Data:\n",
    "The dataset is well maintained and prepared by the Singapore government's open data portal. There is no missing data, which simplifies the preprocessing steps. \n",
    "Load your dataset and prepare it for training. This includes splitting it into training and testing sets and normalizing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Step 2: Prepare the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize continuous features\n",
    "continuous_columns = ['flat_type', 'floor_area_sqm',  'floor', 'remaining_lease_months']\n",
    "binary_columns = df.columns.difference(continuous_columns + ['adjusted_price']).tolist()\n",
    "scaler = StandardScaler()\n",
    "X_train_continuous = scaler.fit_transform(X_train[continuous_columns])\n",
    "X_test_continuous = scaler.transform(X_test[continuous_columns])\n",
    "\n",
    "# Combine scaled continuous features and binary features\n",
    "X_train = np.hstack([X_train_continuous, X_train[binary_columns].values])\n",
    "X_test = np.hstack([X_test_continuous, X_test[binary_columns].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: \n",
    "This step defines a function create_regression_model that constructs a feedforward neural network model for regression using TensorFlow's Keras API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_regression_model(input_shape, params={}):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(shape=input_shape),    \n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: \n",
    "The model is trained using the training data  (\\(X_{\\text{train}}\\) and \\(y_{\\text{train}}\\)). The training process runs for 10 epochs (def: \\ref{def:epoch}) with a batch size of 32. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create and train the model\n",
    "model = create_regression_model(input_shape=[X_train.shape[1]])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, \n",
    "                    validation_data=(X_test, y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5:\n",
    "In the last step, the model is evaluated using various performance metrics and saves the evaluation results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluate the model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "df_results = pd.DataFrame(columns=['Train', 'Test'])\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "df_results.loc['Root Mean Squared Error', 'Train'] = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "df_results.loc['Mean Aboslute Error', 'Train'] = mean_absolute_error(y_train, y_pred)\n",
    "df_results.loc['Mean Aboslute Percentage Error', 'Train'] = mean_absolute_percentage_error(y_train, y_pred)*100\n",
    "df_results.loc['R2 score', 'Train'] = r2_score(y_train, y_pred)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "df_results.loc['Root Mean Squared Error', 'Test'] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "df_results.loc['Mean Aboslute Error', 'Test'] = mean_absolute_error(y_test, y_pred)\n",
    "df_results.loc['Mean Aboslute Percentage Error', 'Test'] = mean_absolute_percentage_error(y_test, y_pred)*100\n",
    "df_results.loc['R2 score', 'Test'] = r2_score(y_test, y_pred)\n",
    "\n",
    "df_results = df_results.astype('Float64').round(2)\n",
    "df_results.to_csv('./data/model_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(y_actual, y_predicted):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_actual, y_predicted, color='blue')\n",
    "    plt.xlabel('Actual value')\n",
    "    plt.ylabel('Predicted value')\n",
    "    plt.title('Predicted vs Actual')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
